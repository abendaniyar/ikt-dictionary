import streamlit as st
import json
import pandas as pd
import base64
import requests
from streamlit.components.v1 import html

# GitHub –±–∞–ø—Ç–∞—É–ª–∞—Ä—ã
GITHUB_TOKEN = st.secrets["GITHUB_TOKEN"]
REPO_OWNER = st.secrets["REPO_OWNER"]
REPO_NAME = st.secrets["REPO_NAME"]
FILE_PATH = "data.json"

headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

@st.cache_data
def load_json_from_github():
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{FILE_PATH}"
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        content = base64.b64decode(res.json()["content"]).decode("utf-8")
        sha = res.json()["sha"]
        return json.loads(content), sha
    else:
        st.error(f"‚ùå GitHub-—Ç–∞–Ω –¥–µ—Ä–µ–∫ –∂“Ø–∫—Ç–µ–ª–º–µ–¥—ñ: {res.status_code}")
        return {}, None
# GitHub-“õ–∞ –∂–∞“£–∞ JSON –∂–∞–∑—É
def update_json_to_github(new_data, sha):
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{FILE_PATH}"
    updated_content = json.dumps(new_data, ensure_ascii=False, indent=2).encode("utf-8")
    b64_content = base64.b64encode(updated_content).decode("utf-8")

    data = {
        "message": "üîÑ –¢–µ—Ä–º–∏–Ω–¥–µ—Ä –∂–∞“£–∞—Ä—Ç—ã–ª–¥—ã",
        "content": b64_content,
        "sha": sha
    }
    response = requests.put(url, headers=headers, json=data)
    if response.status_code == 200 or response.status_code == 201:
        st.success("‚úÖ –¢–µ—Ä–º–∏–Ω–¥–µ—Ä GitHub-—Ç–∞ –∂–∞“£–∞—Ä—Ç—ã–ª–¥—ã!")
    else:
        st.error(f"‚ùå GitHub-“õ–∞ –∂–∞–∑—É “õ–∞—Ç–µ—Å—ñ: {response.text}")

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config("–≠–ª–µ–∫—Ç—Ä–æ–Ω–¥—ã“õ “±“ì—ã–º–¥—ã“õ-—Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è–ª—ã“õ —Å”©–∑–¥—ñ–∫", layout="wide")
st.title("üìò –ê–ö–¢ –∫—É—Ä—Å—ã: –≠–ª–µ–∫—Ç—Ä–æ–Ω–¥—ã“õ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è–ª—ã“õ —Å”©–∑–¥—ñ–∫")

terms, sha = load_json_from_github()

# Excel –∂“Ø–∫—Ç–µ—É
uploaded_file = st.sidebar.file_uploader("üì§ Excel —Ñ–∞–π–ª –∂“Ø–∫—Ç–µ—É (xlsx)", type=["xlsx"])
if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        new_terms = []
        for _, row in df.iterrows():
            term = {
                'kk': row.get('kk', ''),
                'ru': row.get('ru', ''),
                'en': row.get('en', ''),
                'definition': {
                    'kk': row.get('definition_kk', ''),
                    'ru': row.get('definition_ru', ''),
                    'en': row.get('definition_en', '')
                },
                'example': {
                    'kk': row.get('example_kk', ''),
                    'ru': row.get('example_ru', ''),
                    'en': row.get('example_en', '')
                },
                'relations': {
                    'synonyms': str(row.get('relations_synonyms', '')).split(',') if row.get('relations_synonyms') else [],
                    'general_concept': row.get('relations_general_concept', ''),
                    'specific_concepts': str(row.get('relations_specific_concepts', '')).split(',') if row.get('relations_specific_concepts') else [],
                    'associative': str(row.get('relations_associative', '')).split(',') if row.get('relations_associative') else []
                }
            }
            new_terms.append(term)

        lecture_name = st.sidebar.selectbox("üìö “ö–∞–π –¥”ô—Ä—ñ—Å–∫–µ “õ–æ—Å—ã–ª–∞–¥—ã?", list(terms.keys()))
        if st.sidebar.button("‚ûï –¢–µ—Ä–º–∏–Ω–¥–µ—Ä–¥—ñ “õ–æ—Å—É"):
            terms[lecture_name].extend(new_terms)
            with open("data.json", "w", encoding="utf-8") as f:
                json.dump(terms, f, ensure_ascii=False, indent=2)
            st.success(f"‚úÖ {len(new_terms)} –∂–∞“£–∞ —Ç–µ—Ä–º–∏–Ω “õ–æ—Å—ã–ª–¥—ã!")
            st.session_state['selected_term'] = None
            st.experimental_rerun()

    except Exception as e:
        st.error(f"‚ùå Excel –æ“õ—É “õ–∞—Ç–µ—Å—ñ: {e}")
# –Ü–∑–¥–µ—É —Ñ—É–Ω–∫—Ü–∏—è—Å—ã–Ω “õ–æ—Å—É
# --- –Ü–ó–î–ï–£ –ñ”ò–ù–ï –°“Æ–ó–ì–Ü ---
all_terms = []
for lecture, tlist in terms.items():
    for term in tlist:
        all_terms.append(term)

# 1. –ê–≤—Ç–æ–∞—è“õ—Ç–∞—É (autocomplete) –∂”ô–Ω–µ –∫”©–ø—Ç—ñ–ª–¥—ñ —ñ–∑–¥–µ—É
all_titles = list({t.get("kk", "") for t in all_terms} | {t.get("ru", "") for t in all_terms} | {t.get("en", "") for t in all_terms})
search_query = st.text_input("üîç –¢–µ—Ä–º–∏–Ω —ñ–∑–¥–µ—É (kk, ru, en):", value="", placeholder="–º—ã—Å. –∞–ª–≥–æ—Ä–∏—Ç–º, network, –±–∞–∑–∞", help="–ö–µ–∑ –∫–µ–ª–≥–µ–Ω —Ç—ñ–ª–¥–µ —ñ–∑–¥–µ“£—ñ–∑")

# 2. –ê–ª—Ñ–∞–≤–∏—Ç—Ç—ñ–∫ —Å“Ø–∑–≥—ñ
alphabet = sorted(set(term.get("kk", "")[:1].upper() for term in all_terms if term.get("kk", "")))
selected_letter = st.selectbox("üî° ”ò—Ä—ñ–ø –±–æ–π—ã–Ω—à–∞ —Å“Ø–∑–≥—ñ (kk):", ["–ë–∞—Ä–ª—ã“ì—ã"] + alphabet)

filtered_terms = []
for term in all_terms:
    name_kk = term.get("kk", "").lower()
    name_ru = term.get("ru", "").lower()
    name_en = term.get("en", "").lower()

    if search_query.lower() in name_kk or search_query.lower() in name_ru or search_query.lower() in name_en:
        if selected_letter == "–ë–∞—Ä–ª—ã“ì—ã" or name_kk.startswith(selected_letter.lower()):
            filtered_terms.append(term)
    elif not search_query and (selected_letter == "–ë–∞—Ä–ª—ã“ì—ã" or name_kk.startswith(selected_letter.lower())):
        filtered_terms.append(term)

# 3. “∞—Å—ã–Ω—ã—Å—Ç–∞—Ä (recommendations)
if search_query and not filtered_terms:
    from difflib import get_close_matches
    suggestions = get_close_matches(search_query, all_titles, n=5)
    if suggestions:
        st.warning("üõë –ù–∞“õ—Ç—ã —Ç–µ—Ä–º–∏–Ω —Ç–∞–±—ã–ª–º–∞–¥—ã. –ú“Ø–º–∫—ñ–Ω —Å—ñ–∑ –º—ã–Ω–∞–Ω—ã —ñ–∑–¥–µ–¥—ñ“£—ñ–∑:")
        for s in suggestions:
            st.write(f"üëâ {s}")
    else:
        st.info("‚ùó “∞“õ—Å–∞—Å —Ç–µ—Ä–º–∏–Ω–¥–µ—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã.")

# 4. –¢–µ—Ä–º–∏–Ω–¥–µ—Ä–¥—ñ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–ª–∞—É
if filtered_terms:
    st.write(f"### üìã –ù”ô—Ç–∏–∂–µ–ª–µ—Ä: {len(filtered_terms)} —Ç–µ—Ä–º–∏–Ω")
    for term in filtered_terms:
        term_text = f"{term.get('kk', '')} / {term.get('ru', '')} / {term.get('en', '')}"
        st.markdown(f"### üñ• {term_text}")
        
        with st.expander("üìñ –ê–Ω—ã“õ—Ç–∞–º–∞ / Definition"):
            st.markdown(f"**KK:** {term.get('definition', {}).get('kk', '')}")
            st.markdown(f"**RU:** {term.get('definition', {}).get('ru', '')}")
            st.markdown(f"**EN:** {term.get('definition', {}).get('en', '')}")

        with st.expander("üí¨ –ú—ã—Å–∞–ª / Example"):
            st.markdown(f"**KK:** {term.get('example', {}).get('kk', '')}")
            st.markdown(f"**RU:** {term.get('example', {}).get('ru', '')}")
            st.markdown(f"**EN:** {term.get('example', {}).get('en', '')}")

        relations = term.get("relations", {})
        if relations:
            with st.expander("üîó –ë–∞–π–ª–∞–Ω—ã—Å—Ç–∞—Ä"):
                st.write(f"üîÅ –°–∏–Ω–æ–Ω–∏–º–¥–µ—Ä: {', '.join(relations.get('synonyms', []))}")
                st.write(f"üîº –ñ–∞–ª–ø—ã–ª–∞–º–∞: {relations.get('general_concept', '')}")
                st.write(f"üîΩ –ê—Ä–Ω–∞–π—ã: {', '.join(relations.get('specific_concepts', []))}")
                st.write(f"üîó “ö–∞—Ç—ã—Å—Ç—ã: {', '.join(relations.get('associative', []))}")
        st.markdown("---")
else:
    st.info("üìù –¢–µ—Ä–º–∏–Ω —Ç–∞“£–¥–∞“£—ã–∑ –Ω–µ–º–µ—Å–µ —Å“Ø–∑–≥—ñ “õ–æ–ª–¥–∞–Ω—ã“£—ã–∑.")

# –î”ô—Ä—ñ—Å —Ç–∞“£–¥–∞—É—ã
if search_query:
    st.session_state['show_map'] = False
    st.header(f"üîé –Ü–∑–¥–µ—É –Ω”ô—Ç–∏–∂–µ–ª–µ—Ä—ñ: \"{search_query}\"")
    found_terms = []
    for lecture_name, term_list in terms.items():
        for term in term_list:
            if search_query in term.get('kk', '').lower() or search_query in term.get('ru', '').lower() or search_query in term.get('en', '').lower():
                found_terms.append((lecture_name, term))

    if not found_terms:
        st.warning("üõë –ë“±–ª —ñ–∑–¥–µ—É —Å“±—Ä–∞–Ω—ã—Å—ã–Ω–∞ —Å”ô–π–∫–µ—Å —Ç–µ—Ä–º–∏–Ω–¥–µ—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã.")
    else:
        for lecture_name, term in found_terms:
            term_text = f"{term.get('kk', '')} / {term.get('ru', '')} / {term.get('en', '')}"
            st.markdown(f"### üìÇ {lecture_name}<br>üñ• {term_text}", unsafe_allow_html=True)
            speak_buttons(term)

            with st.expander("üìñ –ê–Ω—ã“õ—Ç–∞–º–∞ / –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ / Definition"):
                if 'definition' in term:
                    st.markdown(f"**KK:** {term['definition'].get('kk', '–ñ–æ“õ')}")
                    st.markdown(f"**RU:** {term['definition'].get('ru', '–ù–µ—Ç')}")
                    st.markdown(f"**EN:** {term['definition'].get('en', 'No')}")
                else:
                    st.info("‚ùó –ë“±–ª —Ç–µ—Ä–º–∏–Ω “Ø—à—ñ–Ω –∞–Ω—ã“õ—Ç–∞–º–∞ –±–µ—Ä—ñ–ª–º–µ–≥–µ–Ω.")

            with st.expander("üí¨ –ú—ã—Å–∞–ª / –ü—Ä–∏–º–µ—Ä / Example"):
                if 'example' in term:
                    st.markdown(f"**KK:** {term['example'].get('kk', '–ñ–æ“õ')}")
                    st.markdown(f"**RU:** {term['example'].get('ru', '–ù–µ—Ç')}")
                    st.markdown(f"**EN:** {term['example'].get('en', 'No')}")
                else:
                    st.info("‚ùó –ë“±–ª —Ç–µ—Ä–º–∏–Ω “Ø—à—ñ–Ω –º—ã—Å–∞–ª –±–µ—Ä—ñ–ª–º–µ–≥–µ–Ω.")

            if term.get("image"):
                st.markdown(
                    f'<a href="{term["image"]}" target="_blank">'
                    f'<img src="{term["image"]}" width="200" style="border-radius:10px;" />'
                    f'</a>',
                    unsafe_allow_html=True
                )

            if term.get("source"):
                st.markdown(f"üîó [–î–µ—Ä–µ–∫–∫”©–∑ / –ò—Å—Ç–æ—á–Ω–∏–∫ / Source]({term['source']})")

            st.markdown("---")


# –¢–µ—Ä–º–∏–Ω –º”ô–ª—ñ–º–µ—Ç—ñ
selected = st.session_state.get("selected_term")
if selected:
    for term in terms[lecture]:
        if term.get("kk", "") == selected:
            term_text = f"{term.get('kk', '')} / {term.get('ru', '')} / {term.get('en', '')}"
            st.markdown(f"### üñ• {term_text}")

            st.markdown("**üìñ –ê–Ω—ã“õ—Ç–∞–º–∞:**")
            st.markdown(f"**KK:** {term['definition'].get('kk', '–ñ–æ“õ')}")
            st.markdown(f"**RU:** {term['definition'].get('ru', '–ù–µ—Ç')}")
            st.markdown(f"**EN:** {term['definition'].get('en', 'No')}")

            st.markdown("**üí¨ –ú—ã—Å–∞–ª:**")
            st.markdown(f"**KK:** {term['example'].get('kk', '–ñ–æ“õ')}")
            st.markdown(f"**RU:** {term['example'].get('ru', '–ù–µ—Ç')}")
            st.markdown(f"**EN:** {term['example'].get('en', 'No')}")

            if term.get("image"):
                st.image(term["image"], width=200)
            if term.get("source"):
                st.markdown(f"üîó [–î–µ—Ä–µ–∫–∫”©–∑ / –ò—Å—Ç–æ—á–Ω–∏–∫ / Source]({term['source']})")
